{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpcqgSCTyE-t",
        "outputId": "76fcbe2e-5822-4c25-af9c-0069e829f17d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnk42T-P4exW",
        "outputId": "f3376d89-19e4-4026-e508-cb3e4bc1a1df"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Disable scientific notation for clarity\u001b[39;00m\n\u001b[0;32m      9\u001b[0m np\u001b[38;5;241m.\u001b[39mset_printoptions(suppress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
          ]
        }
      ],
      "source": [
        "from keras.models import load_model\n",
        "from PIL import Image, ImageOps\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "# Disable scientific notation for clarity\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model(\"/content/drive/MyDrive/skin diseases/skin disease model.h5\", compile=False)\n",
        "\n",
        "# Load the class names (labels)\n",
        "class_names = open(\"/content/drive/MyDrive/skin diseases/skin disease labels.txt\", \"r\").readlines()\n",
        "\n",
        "# Define the image size and shape for the model\n",
        "image_size = (224, 224)\n",
        "data = np.ndarray(shape=(1, *image_size, 3), dtype=np.float32)\n",
        "\n",
        "# Input image path\n",
        "image_path = input(\"Insert image path: \")\n",
        "\n",
        "# Preprocess the image\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image = ImageOps.fit(image, image_size, Image.Resampling.LANCZOS)\n",
        "image_array = np.asarray(image)\n",
        "normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
        "data[0] = normalized_image_array\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(data)\n",
        "index = np.argmax(prediction)\n",
        "class_name = class_names[index].strip()  # Remove newline characters\n",
        "confidence_score = prediction[0][index]\n",
        "\n",
        "# Print the prediction and confidence score\n",
        "print(f\"Class: {class_name[2:]}\")  # Skip the first two characters (likely an index or bullet)\n",
        "print(f\"Confidence Score: {confidence_score}\")\n",
        "\n",
        "# Define the URLs for the conditions\n",
        "urls = {\n",
        "    'Acne': 'https://www.mayoclinic.org/diseases-conditions/acne/symptoms-causes/syc-20368047',\n",
        "    'Cellulitis': 'https://www.mayoclinic.org/diseases-conditions/cellulitis/symptoms-causes/syc-20370762',\n",
        "    'Dermatitis': 'https://www.mayoclinic.org/diseases-conditions/dermatitis-eczema/symptoms-causes/syc-20352380',\n",
        "    'Eczema': 'https://www.mayoclinic.org/diseases-conditions/atopic-dermatitis-eczema/symptoms-causes/syc-20353273',\n",
        "    'Ivy': 'https://www.mayoclinic.org/diseases-conditions/poison-ivy/symptoms-causes/syc-20376485',\n",
        "    'Psoriasis': 'https://www.mayoclinic.org/diseases-conditions/psoriasis/symptoms-causes/syc-20355840',\n",
        "    'Scabies': 'https://www.mayoclinic.org/diseases-conditions/scabies/symptoms-causes/syc-20377378'\n",
        "}\n",
        "\n",
        "# Select the appropriate URL based on the predicted class\n",
        "url = urls.get(class_name.split(' ')[-1], 'https://www.mayoclinic.org/diseases-conditions/common-warts/symptoms-causes/syc-20371125')\n",
        "print(url)\n",
        "\n",
        "# Scrape the content from the selected URL\n",
        "r = requests.get(url)\n",
        "if r.status_code == 200:\n",
        "    html = r.text\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Define specific CSS selector for Cellulitis\n",
        "    if 'Cellulitis' in class_name:\n",
        "        target_div = soup.select_one('#__next > div > div > div:nth-child(4) > div.cmp-yspace.cmp-yspace-top-xsm.cmp-yspace-bottom-xsm > div > div > div:nth-child(1) > article > div > div:nth-child(2) > div > div')\n",
        "        if target_div:\n",
        "            content = target_div.get_text(strip=True)\n",
        "        else:\n",
        "            content = \"No content found using the specified CSS selector.\"\n",
        "    else:\n",
        "        # Function to extract text based on section header\n",
        "        def extract_section(header):\n",
        "            section = soup.find(['h2', 'h3', 'h4'], string=header)\n",
        "            if section:\n",
        "                content = []\n",
        "                for sibling in section.find_next_siblings():\n",
        "                    if sibling.name in ['h2', 'h3', 'h4']:\n",
        "                        break\n",
        "                    content.append(sibling.get_text(separator=' ', strip=True))\n",
        "                return ' '.join(content)\n",
        "            return \"Section not found.\"\n",
        "\n",
        "        # Summarization function using transformers\n",
        "        summarizer = pipeline(\"summarization\")\n",
        "\n",
        "        def summarize_text(text, max_length=150, min_length=40):\n",
        "            summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "            return summary[0]['summary_text']\n",
        "\n",
        "        # Extract and summarize specific sections\n",
        "        overview = extract_section(\"Overview\")\n",
        "        symptoms = extract_section(\"Symptoms\")\n",
        "        causes = extract_section(\"Causes\")\n",
        "        risk_factors = extract_section(\"Risk factors\")\n",
        "        if risk_factors == \"Section not found.\":\n",
        "            risk_factors = extract_section(\"Risk Factors\")\n",
        "\n",
        "        # Summarize the extracted sections\n",
        "        if overview != \"Section not found.\":\n",
        "            overview = summarize_text(overview)\n",
        "        if symptoms != \"Section not found.\":\n",
        "            symptoms = summarize_text(symptoms)\n",
        "        if causes != \"Section not found.\":\n",
        "            causes = summarize_text(causes)\n",
        "        if risk_factors != \"Section not found.\":\n",
        "            risk_factors = summarize_text(risk_factors)\n",
        "\n",
        "        # Compile content\n",
        "        content = []\n",
        "        if overview != \"Section not found.\":\n",
        "            content.append(f\"Overview: {overview}\")\n",
        "        if symptoms != \"Section not found.\":\n",
        "            content.append(f\"Symptoms: {symptoms}\")\n",
        "        if causes != \"Section not found.\":\n",
        "            content.append(f\"Causes: {causes}\")\n",
        "        if risk_factors != \"Section not found.\":\n",
        "            content.append(f\"Risk Factors: {risk_factors}\")\n",
        "\n",
        "        content = '\\n'.join(content) if content else \"No content found.\"\n",
        "\n",
        "    # Print the extracted or summarized content\n",
        "    print(content)\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {r.status_code}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
